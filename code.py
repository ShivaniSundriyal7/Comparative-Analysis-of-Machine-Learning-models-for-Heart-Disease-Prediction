# -*- coding: utf-8 -*-
"""heart_Working.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C8gGRrD4eBFZ1LmDc39XBCku91zhoV8j
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn import metrics
from sklearn.preprocessing import  StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report

"""# READING THE FILE"""

df=pd.read_csv('heart.csv')

df.head(10)

df.tail(20)

df.info()

df.shape

df.describe()

"""# DATA CLEANING"""

# Drop the duplicate
df.shape

df=df.drop_duplicates()

df.shape

# Hence it is observed that we had _______ duplicate in the dataset.

df.isnull().sum()

# There is no null value in the dataset

import missingno as mn

mn.bar(df)
plt.show()

df.columns

# no of zeros in the different col
print('No of zeros in the age',df[df['age']==0].shape[0])

print('No of zeros in the trestbps',df[df['trestbps']==0].shape[0])

print('No of zeros in the chol',df[df['chol']==0].shape[0])

print('No of zeros in the thalach',df[df['thalach']==0].shape[0])

df.describe()

f,ax=plt.subplots(1,2,figsize=(10,5))
df['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
# ax[0].set_title('target')
# ax[0].set_ylabel('')
# sns.countplot('target',data=df,ax=ax[1])
# ax[1].set_title('target')
# N,P=df['target'].value_counts()
# print('Negative (0):',N)
# print('Positive (1):',P)
# plt.grid()
plt.show()

"""# DATA VISUALIZATION"""

#Univariate analysis->exploring the data
for i in df.columns:
    if df[i].dtype!="object":
        sns.histplot(x=df[i]);
        plt.show();

plt.figure(figsize=(18,12))
plt.subplot(221)
df["sex"].value_counts().plot.pie(autopct = "%1.0f%%",colors = sns.color_palette("prism",5),startangle = 60,labels=["Male","Female"],
wedgeprops={"linewidth":2,"edgecolor":"k"},explode=[.1,.1],shadow =True)
plt.title("Distribution of Gender")
plt.subplot(222)
ax= sns.distplot(df['age'], rug=True)
plt.title("Age wise distribution")
plt.show()

#As we can see from above plot, in this dataset males percentage is way too higher than females where as an average age of patients is around 58 to 60 years.

"""# Box plot"""

for i in df.columns:
    if df[i].dtype != "object":
        sns.boxplot(y=df[i])
        plt.show()

#we can observe that we have outliers present in trestbps, chol, thalach, oldpeak,ca
#outliers remove
per25 = df['trestbps'].quantile(0.25)
per75 = df['trestbps'].quantile(0.75)
IQR= per75-per25
trestbps_ul = per75 + 1.5*IQR
trestbps_ll = per25 - 1.5*IQR

new_df = df[(df['trestbps'] > trestbps_ll) & (df['trestbps'] < trestbps_ul)]

# sns.boxplot(new_df['trestbps'])

per25 = new_df['chol'].quantile(0.25)
per75 = new_df['chol'].quantile(0.75)
IQR= per75-per25
chol_ul = per75 + 1.5*IQR
chol_ll = per25 - 1.5*IQR

new_df = new_df[(new_df['chol'] > chol_ll) & (new_df['chol'] < chol_ul)]

# sns.boxplot(new_df['chol'])

per25 = new_df['thalach'].quantile(0.25)
per75 = new_df['thalach'].quantile(0.75)
IQR= per75-per25
th_ul = per75 + 1.5*IQR
th_ll = per25 - 1.5*IQR

new_df = new_df[(new_df['thalach'] > th_ll) & (new_df['thalach'] < th_ul)]

# sns.boxplot(new_df['thalach'])

per25 = new_df['oldpeak'].quantile(0.25)
per75 = new_df['oldpeak'].quantile(0.75)
IQR= per75-per25
th_ul = per75 + 1.5*IQR
th_ll = per25 - 1.5*IQR

new_df = new_df[(new_df['oldpeak'] > th_ll) & (new_df['oldpeak'] < th_ul)]

# sns.boxplot(new_df['oldpeak'])

per25 = new_df['ca'].quantile(0.25)
per75 = new_df['ca'].quantile(0.75)
IQR= per75-per25
th_ul = per75 + 1.5*IQR
th_ll = per25 - 1.5*IQR

new_df = new_df[(new_df['ca'] > th_ll) & (new_df['ca'] < th_ul)]

# sns.boxplot(new_df['ca'])

per25 = new_df['thal'].quantile(0.25)
per75 = new_df['thal'].quantile(0.75)
IQR= per75-per25
th_ul = per75 + 1.5*IQR
th_ll = per25 - 1.5*IQR

new_df = new_df[(new_df['thal'] > th_ll) & (new_df['thal'] < th_ul)]

# sns.boxplot(new_df['thal'])

# box plot after removing outliers
for i in new_df.columns:
    if new_df[i].dtype != "object":
        sns.boxplot(y=new_df[i])
        plt.show()

#visualizaing different parameters
new_df.shape

new_df.tail()



"""# Heat map"""

import seaborn as sns
# plot heatmap before
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(),annot=True,cmap="magma",fmt='.2f')

import seaborn as sns
# plot heatmap before
plt.figure(figsize=(12,10))
sns.heatmap(new_df.corr(),annot=True,cmap="magma",fmt='.2f')

f,ax=plt.subplots(1,2,figsize=(10,5))
new_df['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
# ax[0].set_title('target')
# ax[0].set_ylabel('')
# sns.countplot('target',data=df,ax=ax[1])
# ax[1].set_title('target')
# N,P=df['target'].value_counts()
# print('Negative (0):',N)
# print('Positive (1):',P)
# plt.grid()
plt.show()

# the dataset seems to be balanced

x=new_df.iloc[:,:-1].values
y=new_df.iloc[:,-1].values

"""INITIALIZE SAMPLER"""

over_sampler = RandomOverSampler()
x_resampled, y_resampled = over_sampler.fit_resample(x,y)

print(len(x_resampled))

sns.countplot(x=y);

new_df.hist(bins=10,figsize=(10,10))
plt.show()

import seaborn as sns

# plot heatmap
plt.figure(figsize=(12,10))
sns.heatmap(new_df.corr(),annot=True,cmap="magma",fmt='.2f')



# #observation-From correlation heatmap->
# we can see that there is a high correlation between target and
#  ['thalach', 'CP', 'exang', 'oldpeak', 'ca' ]

"""TRAIN TEST SPLIT"""

from sklearn.model_selection import train_test_split

X = new_df.drop(columns='target', axis=1)
Y = new_df['target']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)

X_train.shape,y_train.shape,X_test.shape,y_test.shape

# When you call the train_test_split function with a specific random_state value,
# it ensures that the data is split in the same way every time you run the code with the same random_state value.
# This allows for reproducibility of results, meaning that if you or someone else runs the code with the same random_state value,
# you will obtain the same train-test split.

new_df.head()

"""STANDARDIZATION USING STANDARD SCALING

Splitting the data into training and test data...we use this after seoerating target from the data as target is already in standard form as it is 0/1.
"""

print(new_df.std())

print(X_train.std())

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

print(X_train_scaled.std())

# Apply the same scaling to the testing set
X_test_scaled = scaler.transform(X_test)

print(X_test_scaled.std())

X_train_scaled

"""
1. LOGISTIC REGRESSION

"""

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression(solver='liblinear',multi_class='ovr')
lr.fit(X_train,y_train)

"""MAKING PREDICTION

# Logistic Regression
"""

lr_pred=lr.predict(X_test)

lr_pred.shape

"""TRAIN SCORE AND TEST SCORE"""

# Train score and test score of logistic regression
from sklearn.metrics import accuracy_score

print("Train Accuracy of Logistic Regression",lr.score(X_train,y_train)*100)
print(" Accuracy (Test) of Logistic Regression",lr.score(X_test,y_test)*100)
print("Accuracy score of Logistic Regression",accuracy_score(y_test,lr_pred)*100)

# MAking the Confusion Matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test, lr_pred)

print("Confusion Matrix:")
print(conf_matrix)

# Calculate precision
from sklearn.metrics import precision_score
precision = precision_score(y_test, lr_pred)
print("Precision: ",precision*100)

# Calculate F1 score
from sklearn.metrics import f1_score
f1 = f1_score(y_test, lr_pred)
print("F1 Score: ", f1*100)

# Calculate Recall
from sklearn.metrics import recall_score
recall = recall_score(y_test, lr_pred)
print("Recall :", recall*100)

"""# KNN MODEL"""

from sklearn.neighbors import KNeighborsClassifier

m5 = 'K-NeighborsClassifier'
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train, y_train)
knn_predicted = knn.predict(X_test)
knn_conf_matrix = confusion_matrix(y_test, knn_predicted)
knn_acc_score = accuracy_score(y_test, knn_predicted)
print("confussion matrix")
print(knn_conf_matrix)
print("\n")

print("Accuracy of K-NeighborsClassifier:",knn_acc_score*100,'\n')
print(classification_report(y_test,knn_predicted))
precision = precision_score(y_test, knn_predicted)
print("Precision: ",precision*100)

"""
# Logistic Regression

"""

m1 = 'Logistic Regression'
lr = LogisticRegression()
model = lr.fit(X_train, y_train)
lr_predict = lr.predict(X_test)
lr_conf_matrix = confusion_matrix(y_test, lr_predict)
lr_acc_score = accuracy_score(y_test, lr_predict)
print("confussion matrix")
print(lr_conf_matrix)
print("\n")
print("Train Accuracy of Logistic Regression",lr.score(X_train,y_train)*100)
print("Accuracy of Logistic Regression:",lr_acc_score*100,'\n')
print(classification_report(y_test,lr_predict))

"""# Naive Bayes


"""

from sklearn.naive_bayes import GaussianNB
m2 = 'Naive Bayes'
nb = GaussianNB()
nb.fit(X_train,y_train)
nbpred = nb.predict(X_test)
nb_conf_matrix = confusion_matrix(y_test, nbpred)
nb_acc_score = accuracy_score(y_test, nbpred)
print("confussion matrix")
print(nb_conf_matrix)
print("\n")
print("Accuracy of Naive Bayes model:",nb_acc_score*100,'\n')
print(classification_report(y_test,nbpred))

"""# Support Vector Classifier"""

from sklearn.svm import SVC
m7 = 'Support Vector Classifier'
svc =  SVC(kernel='linear', C=2)
svc.fit(X_train, y_train)
svc_predicted = svc.predict(X_test)
svc_conf_matrix = confusion_matrix(y_test, svc_predicted)
svc_acc_score = accuracy_score(y_test, svc_predicted)
print("confussion matrix")
print(svc_conf_matrix)
print("\n")
print("Accuracy of Support Vector Classifier:",svc_acc_score*100,'\n')
print(classification_report(y_test,svc_predicted))

"""# DecisionTreeClassifier"""

from sklearn.tree import DecisionTreeClassifier
m6 = 'DecisionTreeClassifier'
dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)
dt.fit(X_train, y_train)
dt_predicted = dt.predict(X_test)
dt_conf_matrix = confusion_matrix(y_test, dt_predicted)
dt_acc_score = accuracy_score(y_test, dt_predicted)
print("confussion matrix")
print(dt_conf_matrix)
print("\n")
print("Accuracy of DecisionTreeClassifier:",dt_acc_score*100,'\n')
print(classification_report(y_test,dt_predicted))

"""# Random Forest Classfier"""

from sklearn.ensemble import RandomForestClassifier
m3 = 'Random Forest Classfier'
rf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)
rf.fit(X_train,y_train)
rf_predicted = rf.predict(X_test)
rf_conf_matrix = confusion_matrix(y_test, rf_predicted)
rf_acc_score = accuracy_score(y_test, rf_predicted)
print("confussion matrix")
print(rf_conf_matrix)
print("\n")
print("Accuracy of Random Forest:",rf_acc_score*100,'\n')
print(classification_report(y_test,rf_predicted))